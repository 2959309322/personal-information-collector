# 寒假训练营项目：个人信息聚合助手

## 项目概述

构建一个**个人信息聚合助手**，能够从多个数据源（网页、API、本地文件）采集信息，存储到数据库，并通过 Web API 提供查询服务。

这个项目将帮助你在实战中掌握 Python 编程、网络请求、数据处理、数据库操作和 Web 开发的核心技能。

```
┌─────────────────────────────────────────────────────────────┐
│                    个人信息聚合助手                           │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   数据采集层          数据存储层           服务层              │
│  ┌─────────┐        ┌─────────┐        ┌─────────┐         │
│  │ 爬虫模块 │───────▶│ SQLite/ │───────▶│ FastAPI │         │
│  │ API调用  │        │ Postgres│        │  Web服务 │         │
│  │ 文件解析 │        │  Redis  │        │         │         │
│  └─────────┘        └─────────┘        └─────────┘         │
│       │                  │                  │               │
│       ▼                  ▼                  ▼               │
│  ┌─────────┐        ┌─────────┐        ┌─────────┐         │
│  │ 天气信息 │        │ 数据清洗 │        │ REST API│         │
│  │ 新闻热点 │        │ 定时更新 │        │ 数据查询 │         │
│  │ GitHub  │        │ 缓存策略 │        │ 聚合展示 │         │
│  │ 豆瓣/B站│        │         │        │         │         │
│  └─────────┘        └─────────┘        └─────────┘         │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 学习路线对应关系

| 项目模块  | 对应学习内容                                    |
| ----- | ----------------------------------------- |
| 数据采集  | 阶段1：Python基础、requests、BeautifulSoup、API调用 |
| 数据处理  | 阶段1：Pandas数据清洗、JSON处理、异常处理                |
| 数据存储  | 阶段2：SQLite/PostgreSQL、Redis缓存             |
| Web服务 | 阶段2：FastAPI、RESTful API设计                 |
| 工程化   | 阶段0+1：Git、项目结构、代码规范                       |

---

## 项目分阶段实施

### 第一周：环境搭建 + Python 基础热身

#### 目标

- 完成开发环境配置
- 熟悉 Git 工作流
- 回顾/学习 Python 基础

#### 任务清单

**Day 1-2: 环境搭建**

- [ ] 安装 Python 3.11+、VS Code
- [ ] 配置虚拟环境（推荐 conda 或 venv）
- [ ] 安装必要依赖：requests, beautifulsoup4, pandas, fastapi, uvicorn, sqlalchemy
- [ ] 在 GitHub 创建项目仓库，配置 .gitignore、README.md
- [ ] 学习 Git 基本操作：clone, add, commit, push, pull

**Day 3-4: Python 热身练习**

- [ ] 练习1：实现数据聚合函数（练习 dict、list 操作）
- [ ] 练习2：实现配置文件读写（练习文件 I/O、JSON 处理）
- [ ] 练习3：实现计时装饰器和重试装饰器（理解装饰器原理）
                就是一个是有参装饰，一个是无参装饰

**Day 5: 项目初始化**

- [ ] 创建项目目录结构
- [ ] 编写项目 README
- [ ] 第一次正式 commit

**项目目录结构参考**：

```
info-aggregator/
├── README.md
├── requirements.txt
├── .gitignore
├── config/
│   └── settings.py          # 配置管理
├── src/
│   ├── collectors/           # 数据采集模块
│   │   ├── base.py          # 采集器基类
│   │   ├── weather.py       # 天气采集
│   │   ├── news.py          # 新闻采集
│   │   └── github.py        # GitHub 采集
│   ├── processors/           # 数据处理模块
│   │   └── cleaner.py       # 数据清洗
│   ├── storage/              # 数据存储模块
│   │   ├── database.py      # 数据库操作
│   │   └── cache.py         # 缓存操作
│   └── api/                  # Web API 模块
│       ├── main.py          # FastAPI 入口
│       └── routes/          # 路由定义
├── tests/                    # 测试目录
└── scripts/                  # 脚本目录
    └── init_db.py           # 数据库初始化
```

---

### 第二周：数据采集模块

#### 目标

- 掌握 HTTP 请求与响应处理
- 学会使用 requests 库
- 掌握网页解析技术
- 理解 API 调用方式

#### 任务清单

**Day 1-2: 天气信息采集（API 调用）**

从公开天气 API 获取数据：

- [ ] 注册免费天气 API（和风天气 / OpenWeatherMap）
- [ ] 实现单城市天气查询
- [ ] 实现多城市批量查询
- [ ] 处理 API 异常（超时、限流、错误响应）

**学习要点**：

- requests.get() 的使用
- 请求参数（params）的传递
- 响应状态码检查
- JSON 数据解析
- 异常处理（网络超时、API 错误等）

**Day 3-4: 新闻热点采集（网页爬虫）**

爬取热点信息（选择 1-2 个实现）：

- [ ] 知乎热榜
- [ ] 微博热搜
- [ ] GitHub Trending
- [ ] B站热门

**学习要点**：

- 请求头（Headers）的作用和设置
- BeautifulSoup 选择器（CSS 选择器、find/find_all）
- 处理反爬（User-Agent、延时请求）
- 数据清洗（去除空白、格式化）

**Day 5: 采集器抽象与统一**

- [ ] 设计采集器基类（定义统一接口）
- [ ] 统一数据格式（定义数据类）
- [ ] 实现带异常处理的安全采集方法

---

### 第三周：数据存储模块

#### 目标

- 掌握 SQL 基础与数据库设计
- 学会使用 SQLAlchemy ORM
- 理解缓存的作用与使用

#### 任务清单

**Day 1-2: 数据库设计与基础操作**

- [ ] 设计数据表结构（天气表、新闻表）
- [ ] 使用 SQLAlchemy 定义模型
- [ ] 实现 CRUD 操作（增删改查）
- [ ] 实现按条件查询（按城市、按来源、按时间范围）

**学习要点**：

- 数据库表设计（字段类型、主键、索引）
- SQLAlchemy 模型定义
- Session 管理与事务处理
- 查询优化（索引、limit）

**Day 3-4: Redis 缓存**

- [ ] 安装并连接 Redis
- [ ] 实现基本的 get/set 操作
- [ ] 实现带过期时间的缓存
- [ ] 实现"缓存不存在则查询并缓存"的模式

**学习要点**：

- Redis 基本数据类型（String）
- 缓存过期策略
- 缓存的使用场景
- JSON 序列化与反序列化

**Day 5: 数据聚合服务**

- [ ] 整合采集器和存储模块
- [ ] 实现数据刷新逻辑（采集 → 清洗 → 存储 → 清缓存）
- [ ] 实现仪表盘数据聚合（多数据源合并展示）

---

### 第四周：Web API 开发

#### 目标

- 掌握 FastAPI 框架使用
- 理解 RESTful API 设计
- 学会 API 文档编写

#### 任务清单

**Day 1-2: FastAPI 基础**

实现以下 API 接口：

- [ ] `GET /` - API 欢迎页
- [ ] `GET /weather/{city}` - 获取指定城市天气
- [ ] `GET /weather?cities=北京,上海` - 批量获取天气
- [ ] `GET /news?source=zhihu&limit=10` - 获取新闻列表
- [ ] `GET /dashboard` - 获取仪表盘聚合数据
- [ ] `POST /refresh` - 手动刷新数据

**学习要点**：

- 路由定义与路径参数
- 查询参数与请求体
- Pydantic 模型定义（请求/响应模型）
- 自动生成 API 文档（/docs）

**Day 3-4: 完善 API**

- [ ] 添加请求日志中间件
- [ ] 实现统一的错误处理
- [ ] 添加请求参数校验
- [ ] 优化响应格式

**学习要点**：

- FastAPI 中间件机制
- 异常处理器
- 参数校验（Query、Path 的约束）
- 响应模型与状态码

**Day 5: 测试与文档**

- [ ] 编写 API 测试用例
- [ ] 完善 README 文档
- [ ] 整理代码，最终提交

---

## 进阶挑战（可选）

完成基础功能后，可以尝试以下进阶任务：

### 挑战 1：定时任务

使用 APScheduler 实现数据自动定时刷新（如每 30 分钟更新一次）

### 挑战 2：更多数据源

添加更多你感兴趣的数据源：

- 豆瓣电影/图书
- Steam 游戏折扣
- 掘金/CSDN 技术热文
- 股票/基金数据

### 挑战 3：数据可视化

添加一个简单的前端页面，用图表展示聚合数据

### 挑战 4：Go 语言重写

尝试用 Go + Gin 重写部分 API，体验不同语言的开发风格

---

## 提交要求

### 代码仓库

- [ ] 完整的 README（项目介绍、安装步骤、使用说明）
- [ ] 清晰的目录结构
- [ ] 规范的 commit 记录（建议使用 Conventional Commits）
- [ ] requirements.txt 依赖清单

### 功能要求

- [ ] 至少实现 2 个数据源的采集
- [ ] 数据能够持久化存储到数据库
- [ ] 提供 RESTful API 接口
- [ ] API 文档可访问（FastAPI 自带的 /docs）

### 文档要求

- [ ] 记录学习过程中遇到的问题和解决方案
- [ ] 简单的 API 使用示例

---

## 一些建议

- **善用 AI 工具**：遇到不会的可以问 AI，但要理解它给的代码
- **先跑通再优化**：不要追求一次写出完美代码，先实现功能再迭代
- **多写注释**：方便自己回顾，也方便师兄 review
- **及时 commit**：小步提交，养成好习惯
- **遇到问题别死磕**：卡住超过 30 分钟就换个思路或者问人

加油！有问题随时在群里问。
